<h2>Bigram Transformer Model</h2>
This repository contains a transformer-based text generation model, designed to mimic the style and content of its training data built from scratch. This model leverages bigrams as its fundamental building blocks, and is structured with multi-headed attention blocks, layer normalization, dropout, and feedforward layers to enhance learning and generalization. This model offers a starting framework for experimenting with various textual styles and formats.
<br /><br />
<em>*Please ensure to add your own input file to the current working directory</em>
<br />
<br />

To test generation based on your own input data run the following (after cloning this repo): <br />
```python train.py [INPUT_FILE.txt]```
