<h2>Bigram Transformer Model</h2>
This repository contains a character-level pre-trained transformer-based text generation model, designed to mimic the style and content of its training data (Shakespeare) built from scratch. This model leverages bigrams as its fundamental building blocks, and is structured with multi-headed attention blocks, layer normalization, dropout, and feedforward layers to enhance learning and generalization. This model offers a starting framework for experimenting with various textual styles and formats.<br/><br/>

To run this program, download the required modules with: <br />
```python train.py```

<br /><br />
If you would like to train this model on your own dataset, please add a .txt file with the contents you would like to mimic. <br/>
<em>*Add the input file to the current working directory</em> </br>
<em>*Ensure that input.txt (from git pull) remains unchanged</em>
<br />
<br />

To run the program run the following and follow the prompts (after cloning this repo): <br />
```python train.py```
