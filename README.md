Bigram Transformer Model<br />
This repository contains a transformer-based text generation model, designed to mimic the style and content of its training data. This model leverages bigrams as its fundamental building blocks, and is structured with multi-headed attention blocks, layer normalization, dropout, and feedforward layers to enhance learning and generalization. This model offers a starting framework for experimenting with various textual styles and formats.
<br /><br />
Please ensure to add your own input file to the current working directory
<br />

To test generation based on your own input data run the following (after cloning this repo): <br />
```python train.py [INPUT_FILE.txt]```
